{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#cudf for linux\n",
    "# %conda install -c rapidsai -c conda-forge -c nvidia cudf=23.04 python=3.10 cudatoolkit=11.8\n",
    "#https://github.com/rapidsai/cudf\n",
    "#https://docs.rapids.ai/install#WSL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import cudf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to df\n",
    "## Load data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "#Gowalla setting\n",
    "#================================================\n",
    "DATA_NAME = \"Gowalla\"\n",
    "FILE_NAME = \"Gowalla_totalCheckins.txt\"\n",
    "COL_NAMES = ['user','time','lat','long','item']\n",
    "BASE_DATA_RATIO = 7# N/10\n",
    "INC_RATIO = 3\n",
    "INC_STEP = 5\n",
    "#================================================\n",
    "\n",
    "# config\n",
    "ROOT_PATH = os.path.abspath(\"\")\n",
    "DATA_PATH = join(ROOT_PATH+\"\\\\data\", DATA_NAME)\n",
    "SAVE_PATH = join(ROOT_PATH,\"data\")\n",
    "FILE_PATH = join(DATA_PATH, FILE_NAME)\n",
    "FILT_THRESHOLD = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load File\n",
    "origin_df = pd.read_table(FILE_PATH, names=COL_NAMES)\n",
    "#origin_df = cudf.from_pandas(origin_df)\n",
    "def print_info(df):\n",
    "        print(f\"Total Edges : {len(df)}\\nTotal User : {len(df['user'].unique())}\\nTotal item : {len(df['item'].unique())} \\\n",
    "                \\nSmallest user id : {df['user'].unique().min()} \\\n",
    "                \\nbiggest user id : {df['user'].unique().max()} \\\n",
    "                \\nSmallest item id : {df['item'].unique().min()} \\\n",
    "                \\nbiggest item id : {df['item'].unique().max()} \\\n",
    "                \\nMin Interaction Per user : {df.user.value_counts().min()} \\\n",
    "                \\nMax Interaction Per user : {df.user.value_counts().max()} \\\n",
    "                \\nAvg Interaction Per user : {df.user.value_counts().mean()}\\\n",
    "                \\nMin Interaction Per item : {df.item.value_counts().min()} \\\n",
    "                \\nMax Interaction Per item : {df.item.value_counts().max()} \\\n",
    "                \\nAvg Interaction Per item : {df.item.value_counts().mean()}\")\n",
    "print_info(origin_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음번 부터는 필터링 하고 리팩토링 할것 필요 없는 것들 까지들어가있어서 시간이 훨씬 오래걸린다......"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반복문이 아닌 판다스 기본 기능을 활용하니까 훨씬 빠르다</br>\n",
    "mapping = {index: i for i, index in enumerate(df.index.unique())}</br>\n",
    "이 방법으로 훨씬 간단하게 매핑 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refactoring_from_0(df):\n",
    "    out_df = pd.DataFrame() \n",
    "    \n",
    "    original_uid = np.sort(df['user'].unique())\n",
    "    original_iid = np.sort(df['item'].unique())\n",
    "\n",
    "    u_range = range(len(original_uid))\n",
    "    i_range = range(len(original_iid))\n",
    "\n",
    "    uid_mapping = { o_id: n_id for o_id, n_id in zip(original_uid, u_range)} # 원래 유저 아이디 (중간중간 비어있음) : 순서대로 유저 아이디\n",
    "    iid_mapping = { o_id: n_id for o_id, n_id in zip(original_iid,i_range)} # 원래 아이템 아이디 : 순서대로 아이템 아이디\n",
    "\n",
    "    uid_map = pd.DataFrame({'o_id' : list(uid_mapping.keys()), 'n_id' : list(uid_mapping.values())})\n",
    "\n",
    "    iid_map = pd.DataFrame({'o_id' : list(iid_mapping.keys()),'n_id':list(iid_mapping.values())})\n",
    "\n",
    "\n",
    "    out_df['user'] = df['user'].map(uid_mapping)\n",
    "    out_df['item'] = df['item'].map(iid_mapping)\n",
    "    out_df['time'] = df['time']\n",
    "    return out_df, uid_map, iid_map\n",
    "\n",
    "\n",
    "# NUM_CORES = 8\n",
    "\n",
    "# def user_replace(df):\n",
    "#     return df.replace({'user':{user_id:id}})\n",
    "\n",
    "# def item_replace(df):\n",
    "#     return df.replace({'item':{item_id:id}})\n",
    "\n",
    "\n",
    "# def parallelize_dataframe(df, func):\n",
    "#     df_split = np.array_split(df, NUM_CORES)\n",
    "#     pool = Pool(NUM_CORES)\n",
    "#     df = pd.concat(pool.map(func, df_split))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return df\n",
    "\n",
    "# Change each user and item index start from 0\n",
    "# origin_df.sort_values(by='user',ascending=True)\n",
    "\n",
    "# # Change user id to start from 0\n",
    "# id = 0\n",
    "\n",
    "\n",
    "# for user_id in tqdm(origin_df['user'].unique()):\n",
    "#     parameter = {'user':{user_id:id}}\n",
    "#     # origin_df = parallelize_dataframe(origin_df, user_replace)\n",
    "#     id += 1\n",
    "\n",
    "# Change item id to start from 0\n",
    "# origin_df.sort_values(by='item',ascending=True)\n",
    "# id = 0\n",
    "\n",
    "# for item_id in tqdm(origin_df['item'].unique()):\n",
    "#     parameter = {'item':{item_id:id}}\n",
    "#     # origin_df = parallelize_dataframe(origin_df, item_replace)\n",
    "#     id += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Test refactoring function \n",
    "temp_df, a, b = refactoring_from_0(origin_df)\n",
    "print_info(temp_df)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Save refactored dataframe\n",
    "temp_df.to_csv(DATA_PATH+\"\\\\refactored.intr\",sep=\" \",index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load refactored dataframe\n",
    "origin_df = pd.read_table(DATA_PATH+\"\\\\refactored.intr\", names=COL_NAMES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "1. remove duplcaite\n",
    "2. filter data which have under 10 interaction\n",
    "3. refactor the id of user and item start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = origin_df\n",
    "# Just interaction and drop duplicate\n",
    "df = df[['user','time','item']]\n",
    "df = df.drop_duplicates(subset=['user','item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Save Interaction information without filtering\n",
    "WO_filtering = df\n",
    "# Sort with ascending order with user first and item\n",
    "WO_filtering = WO_filtering.sort_values(by=['user','item','time'], ascending=[True,True,True])\n",
    "WO_filtering, a, b = refactoring_from_0(WO_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "WO_filtering_filename = DATA_PATH + \"\\\\WO_Filtering.intr\"\n",
    "if not os.path.exists(WO_filtering_filename):\n",
    "    WO_filtering.to_csv(WO_filtering_filename,sep=\" \", index=False, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data that has less than 10 interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "filtering_filename = DATA_PATH + \"\\\\filtering_refactored.intr\"\n",
    "fdf = df\n",
    "\n",
    "while fdf.user.value_counts().min() < FILT_THRESHOLD or fdf.item.value_counts().min() < FILT_THRESHOLD:\n",
    "    df_item = fdf.groupby('item').count()\n",
    "    df_item = df_item[df_item.user < FILT_THRESHOLD]\n",
    "    li = df_item.index.to_list()\n",
    "    fdf = fdf.drop(fdf.loc[fdf.item.isin(li)].index)\n",
    "    # print_info(fdf)\n",
    "    df_usr = fdf.groupby('user').count()\n",
    "    df_usr = df_usr[df_usr.item < FILT_THRESHOLD]\n",
    "    li = df_usr.index.to_list()\n",
    "    fdf = fdf.drop(fdf.loc[fdf.user.isin(li)].index)\n",
    "    # print_info(fdf)\n",
    "    # print(f\"Total Edges : {len(fdf)}\\nTotal User : {len(fdf['user'].unique())}\\nTotal item : {len(fdf['item'].unique())} \\\n",
    "    #             \\nMin Interaction Per user : {fdf.user.value_counts().min()} \\\n",
    "    #             \\nMax Interaction Per user : {fdf.user.value_counts().max()} \\\n",
    "    #             \\nAvg Interaction Per user : {fdf.user.value_counts().mean()}\\\n",
    "    #             \\nMin Interaction Per item : {fdf.item.value_counts().min()} \\\n",
    "    #             \\nMax Interaction Per item : {fdf.item.value_counts().max()} \\\n",
    "    #             \\nAvg Interaction Per item : {fdf.item.value_counts().mean()}\")\n",
    "print_info(fdf)\n",
    "fdf = fdf.reset_index().drop(columns = ['index'])\n",
    "fdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor to make each user and item number start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fdf, filtered_uid_map, filtered_iid_map = refactoring_from_0(fdf)\n",
    "print_info(fdf)\n",
    "fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fdf.to_csv(filtering_filename,sep=\" \", index=False, header=None)\n",
    "filtered_uid_map.to_csv(DATA_PATH+\"\\\\filtered_uid_map.uid\",sep=\" \", index=False, header=None)\n",
    "filtered_iid_map.to_csv(DATA_PATH+\"\\\\filtered_iid_map.iid\",sep=\" \", index=False, header=None)\n",
    "shuffled = pd.DataFrame(np.random.permutation(fdf))\n",
    "train = shuffled[:len(fdf)*8//10]\n",
    "test = shuffled[len(fdf)*8//10:]\n",
    "train.to_csv(DATA_PATH+\"\\\\train.data\",sep=\" \", index=False, header=None)\n",
    "test.to_csv(DATA_PATH+\"\\\\test.data\",sep=\" \", index=False, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in to train, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item Time Sequential Incremental Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027464\n",
      "821971\n",
      "Inc0 Block Size : 41098\n",
      "Inc1 Block Size : 41098\n",
      "Inc2 Block Size : 41098\n",
      "Inc3 Block Size : 41098\n",
      "Inc4 Block Size : 41101\n"
     ]
    }
   ],
   "source": [
    "INC_TIME_FILE_PATH = os.path.join(SAVE_PATH,f\"Incremental\\\\{DATA_NAME}\\\\Time\\\\\")\n",
    "fdf = fdf.sort_values(by=\"user\", ascending=True)\n",
    "print(len(fdf))\n",
    "pivot = len(fdf)*BASE_DATA_RATIO//10\n",
    "fdf[:pivot].to_csv(INC_TIME_FILE_PATH + \"base.data\", sep=\" \", header=None, index=False)\n",
    "print(pivot)\n",
    "remain = fdf[pivot:]\n",
    "start = 0\n",
    "length = (len(fdf) - pivot)//INC_STEP\n",
    "inc_block = []\n",
    "for i in range(INC_STEP):\n",
    "    if i != INC_STEP-1:\n",
    "        inc_block.append(remain[start:start+length])\n",
    "    else:\n",
    "        inc_block.append(remain[start:])\n",
    "    start += length\n",
    "    print(f\"Inc{i} Block Size : {len(inc_block[i])}\")\n",
    "    inc_block[i].to_csv(INC_TIME_FILE_PATH+f\"inc{i+1}.data\", sep = \" \", header=None, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total User = 29858\n",
      "Base Block Size : 822003\n",
      "Base Done\n",
      "Inc1 Done\n",
      "Inc Block1 Size : 41111\n",
      "Inc2 Done\n",
      "Inc Block2 Size : 41119\n",
      "Inc3 Done\n",
      "Inc Block3 Size : 41122\n",
      "Inc4 Done\n",
      "Inc Block4 Size : 41131\n",
      "Inc5 Done\n",
      "Inc Block5 Size : 40978\n"
     ]
    }
   ],
   "source": [
    "INC_USER_FILE_PATH = os.path.join(SAVE_PATH,f\"Incremental\\\\{DATA_NAME}\\\\User\\\\\")\n",
    "print(f\"Total User = {len(fdf['user'].unique())}\")\n",
    "# Make df containing number of user interaction\n",
    "ucdf = fdf.groupby('user').count()\n",
    "# Shuffle the df\n",
    "ucdf = ucdf.iloc[np.random.permutation(ucdf.index)].reset_index()\n",
    "\n",
    "is_base = True\n",
    "\n",
    "base_user = []\n",
    "base_size = len(fdf)*BASE_DATA_RATIO//10\n",
    "inc_user = [[] for i in range(INC_STEP)]\n",
    "inc_size = len(fdf)*INC_RATIO\n",
    "\n",
    "i = 0\n",
    "cur = 0\n",
    "\n",
    "while i < INC_STEP:\n",
    "    count = 0\n",
    "    if is_base:\n",
    "        # Base block\n",
    "        while count < base_size:\n",
    "            row = ucdf.iloc[cur]\n",
    "            base_user.append(row['user'])\n",
    "            count += row['item']\n",
    "            cur += 1\n",
    "        print(f\"Base Block Size : {count}\")\n",
    "        with open(INC_USER_FILE_PATH+\"base_user.data\", 'w') as fp:\n",
    "            for user in base_user:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % user)\n",
    "        print('Base Done')\n",
    "        is_base = False\n",
    "    elif i == INC_STEP-1:\n",
    "        # Last incremental block\n",
    "        inc_user[i]= ucdf[cur:]['user'].values.tolist()\n",
    "        count = ucdf[cur:]['item'].values.sum()\n",
    "        with open(INC_USER_FILE_PATH+f\"inc_user_{i+1}.data\", 'w') as fp:\n",
    "            for user in inc_user[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % user)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1\n",
    "    else:\n",
    "        # Incremental block\n",
    "        while count <= inc_size:\n",
    "            row = ucdf.iloc[cur]\n",
    "            inc_user[i].append(row['user'])\n",
    "            count += row['item']\n",
    "            cur += 1\n",
    "        with open(INC_USER_FILE_PATH+f\"inc_user_{i+1}.data\", 'w') as fp:\n",
    "            for user in inc_user[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % user)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Item = 40988\n",
      "Base Block Size : 821989\n",
      "Base Done\n",
      "Inc1 Done\n",
      "Inc Block1 Size : 41273\n",
      "Inc2 Done\n",
      "Inc Block2 Size : 41161\n",
      "Inc3 Done\n",
      "Inc Block3 Size : 41112\n",
      "Inc4 Done\n",
      "Inc Block4 Size : 41107\n",
      "Inc5 Done\n",
      "Inc Block5 Size : 40822\n"
     ]
    }
   ],
   "source": [
    "INC_ITEM_FILE_PATH = os.path.join(SAVE_PATH,f\"Incremental\\\\{DATA_NAME}\\\\Item\\\\\")\n",
    "print(f\"Total Item = {len(fdf['item'].unique())}\")\n",
    "# Make df containing number of user interaction\n",
    "icdf = fdf.groupby('item').count()\n",
    "# Shuffle the df\n",
    "icdf = icdf.iloc[np.random.permutation(icdf.index)].reset_index()\n",
    "\n",
    "is_base = True\n",
    "\n",
    "base_item = []\n",
    "inc_item = [[] for i in range(INC_STEP)]\n",
    "\n",
    "i = 0\n",
    "cur = 0\n",
    "\n",
    "while i < INC_STEP:\n",
    "    count = 0\n",
    "    if is_base:\n",
    "        # Base block\n",
    "        while count < base_size:\n",
    "            row = icdf.iloc[cur]\n",
    "            base_item.append(row['item'])\n",
    "            count += row['user']\n",
    "            cur += 1\n",
    "        print(f\"Base Block Size : {count}\")\n",
    "        with open(INC_ITEM_FILE_PATH+\"base_item.data\", 'w') as fp:\n",
    "            for item in base_user:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "        print('Base Done')\n",
    "        is_base = False\n",
    "    elif i == INC_STEP-1:\n",
    "        # Last incremental block\n",
    "        inc_item[i]= icdf[cur:]['item'].values.tolist()\n",
    "        count = icdf[cur:]['user'].values.sum()\n",
    "        with open(INC_ITEM_FILE_PATH+f\"inc_item_{i+1}.data\", 'w') as fp:\n",
    "            for item in inc_item[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1\n",
    "    else:\n",
    "        # Incremental block\n",
    "        while count <= inc_size:\n",
    "            row = icdf.iloc[cur]\n",
    "            inc_item[i].append(row['item'])\n",
    "            count += row['user']\n",
    "            cur += 1\n",
    "        with open(INC_ITEM_FILE_PATH+f\"inc_item_{i+1}.data\", 'w') as fp:\n",
    "            for item in inc_item[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_Notebook",
   "language": "python",
   "name": "jupyter_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
