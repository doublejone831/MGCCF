{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from enum import Enum\n",
    "# from parse import parse_args\n",
    "import multiprocessing\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# args = parse_args()\n",
    "\n",
    "# ROOT_PATH = os.path.dirname(os.path.dirname(__file__))\n",
    "# CODE_PATH = join(ROOT_PATH, 'code')\n",
    "# DATA_PATH = join(ROOT_PATH, 'data')\n",
    "# BOARD_PATH = join(CODE_PATH, 'runs')\n",
    "# FILE_PATH = join(CODE_PATH, 'checkpoints')\n",
    "# import sys\n",
    "# sys.path.append(join(CODE_PATH, 'sources'))\n",
    "\n",
    "\n",
    "# if not os.path.exists(FILE_PATH):\n",
    "#     os.makedirs(FILE_PATH, exist_ok=True)\n",
    "\n",
    "config = {}\n",
    "all_dataset = ['lastfm', 'gowalla', 'yelp2018', 'amazon-book']\n",
    "all_models  = ['mf', 'lgn']\n",
    "# config['batch_size'] = 4096\n",
    "config['bpr_batch_size'] = 2048 # the batch size for bpr loss training procedure\"\n",
    "config['latent_dim_rec'] = 64 # the embedding size of lightGCN\n",
    "config['lightGCN_n_layers']= 3 # the layer num of lightGCN\n",
    "config['dropout'] = 0 # using the dropout or not\n",
    "config['keep_prob']  = 0.6 # the batch size for bpr loss training procedure\n",
    "config['A_n_fold'] = 100 # the fold num used to split large adj matrix, like gowalla\n",
    "config['test_u_batch_size'] = 100 # the batch size of users for testing\n",
    "config['multicore'] = 0 # whether we use multiprocessing or not in test\n",
    "config['lr'] = 0.001 # the learning rate\n",
    "config['decay'] = 1e-4 # the weight decay for l2 normalizaton\n",
    "config['pretrain'] = 0 # whether we use pretrained weight or not\n",
    "config['A_split'] = False #\n",
    "config['bigdata'] = False #\n",
    "\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n",
    "CORES = multiprocessing.cpu_count() // 2\n",
    "seed = 2020\n",
    "\n",
    "dataset = 'gowalla'\n",
    "model_name = 'lgn'\n",
    "if dataset not in all_dataset:\n",
    "    raise NotImplementedError(f\"Haven't supported {dataset} yet!, try {all_dataset}\")\n",
    "if model_name not in all_models:\n",
    "    raise NotImplementedError(f\"Haven't supported {model_name} yet!, try {all_models}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_epochs = 1000\n",
    "LOAD = 0\n",
    "PATH = \"./checkpoints\" # path to save weights\n",
    "topks = eval(\"[20]\") # @k test list\n",
    "tensorboard = 1 # enable tensorboard\n",
    "comment = \"lgn\"\n",
    "# let pandas shut up\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "def cprint(words : str):\n",
    "    print(f\"\\033[0;30;43m{words}\\033[0m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "# abstracted class for every data\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"init dataset\")\n",
    "    \n",
    "    # @property - getter / @variable.setter - setter\n",
    "    @property\n",
    "    def n_users(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def allPos(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserPosItems(self, users):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserNegItems(self, users):\n",
    "        \"\"\"\n",
    "        not necessary for large dataset\n",
    "        it's stupid to return all neg items in super large dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getSparseGraph(self):\n",
    "        \"\"\"\n",
    "        build a graph in torch.sparse.IntTensor.\n",
    "        Details in NGCF's matrix form\n",
    "        A = \n",
    "            |I,   R|\n",
    "            |R^T, I|\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gowalla(BasicDataset):\n",
    "    \"\"\" \n",
    "    Dataset type for pytorch include graph information\n",
    "    Gowalla Dataset\n",
    "    \n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_notebook",
   "language": "python",
   "name": "jupyter_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
