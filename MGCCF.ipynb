{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "torch : 1.13.1</br>\n",
    "cuda : 11.7</br>\n",
    "torch_geometric : 2.3.0</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "     ---------------------------------------- 10.0/10.0 MB 9.4 MB/s eta 0:00:00\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "     ------------------------------------- 502.3/502.3 kB 30.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pc\\.conda\\envs\\jupyter_notebook\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pc\\.conda\\envs\\jupyter_notebook\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\.conda\\envs\\jupyter_notebook\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.3.5 pytz-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "# %pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\n",
    "# %pip install torch_geometric\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "11.7\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from enum import Enum\n",
    "# from parse import parse_args\n",
    "import multiprocessing\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# args = parse_args()\n",
    "\n",
    "# ROOT_PATH = os.path.dirname(os.path.dirname(__file__))\n",
    "# CODE_PATH = join(ROOT_PATH, 'code')\n",
    "# DATA_PATH = join(ROOT_PATH, 'data')\n",
    "# BOARD_PATH = join(CODE_PATH, 'runs')\n",
    "# FILE_PATH = join(CODE_PATH, 'checkpoints')\n",
    "# import sys\n",
    "# sys.path.append(join(CODE_PATH, 'sources'))\n",
    "\n",
    "\n",
    "# if not os.path.exists(FILE_PATH):\n",
    "#     os.makedirs(FILE_PATH, exist_ok=True)\n",
    "\n",
    "config = {}\n",
    "all_dataset = ['lastfm', 'gowalla', 'yelp2018', 'amazon-book']\n",
    "all_models  = ['MMGCF']\n",
    "# config['batch_size'] = 4096\n",
    "config['emb_size'] = 64\n",
    "config['bpr_batch_size'] = 1024 # the batch size for bpr loss training procedure\"\n",
    "config['latent_dim_rec'] = 64 # the embedding size\n",
    "config['n_layers']= 2 # the layer num \n",
    "config['dropout'] = 0 # using the dropout or not\n",
    "config['keep_prob']  = 0.6 # the batch size for bpr loss training procedure\n",
    "config['A_n_fold'] = 100 # the fold num used to split large adj matrix, like gowalla\n",
    "config['test_u_batch_size'] = 100 # the batch size of users for testing\n",
    "config['multicore'] = 0 # whether we use multiprocessing or not in test\n",
    "config['lr'] = 0.001 # the learning rate\n",
    "config['decay'] = 1e-4 # the weight decay for l2 normalizaton\n",
    "config['pretrain'] = 0 # whether we use pretrained weight or not\n",
    "config['A_split'] = False #\n",
    "config['bigdata'] = False #\n",
    "\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n",
    "CORES = multiprocessing.cpu_count() // 2\n",
    "seed = 2020\n",
    "\n",
    "dataset = 'gowalla'\n",
    "model_name = 'lgn'\n",
    "if dataset not in all_dataset:\n",
    "    raise NotImplementedError(f\"Haven't supported {dataset} yet!, try {all_dataset}\")\n",
    "if model_name not in all_models:\n",
    "    raise NotImplementedError(f\"Haven't supported {model_name} yet!, try {all_models}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_epochs = 1000\n",
    "LOAD = 0\n",
    "PATH = \"./checkpoints\" # path to save weights\n",
    "topks = eval(\"[20]\") # @k test list\n",
    "tensorboard = 1 # enable tensorboard\n",
    "comment = \"lgn\"\n",
    "# let pandas shut up\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class Gowalla(object):\n",
    "    \"\"\"\n",
    "    To load the Gowalla data\n",
    "    Current Version could be used as abstrct class after\n",
    "    Gowalla only contains\n",
    "    edge : \"user\", \"item\", \"timestamp\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Gowalla,self).__init__()\n",
    "    \n",
    "\n",
    "    def load_data(self, path, User_feature_path = None, Item_feature_path = None):\n",
    "        \"\"\" \n",
    "        Load the data from file path\n",
    "        Parameter:\n",
    "        train_path : str => file path for train data set\n",
    "        test_path : str => file path for test data set\n",
    "        User_feature_path : str , default = None => file path for user data \n",
    "        Item_feature_path : str , default = None => file path for item data \n",
    "        \"\"\"\n",
    "        if os.path.exists(path):\n",
    "            self.edges = pd.read_table(path, names=[\"user\",\"item\",\"timestamp\"], sep=\" \",dtype={'user':np.int64,'item':np.int64})\n",
    "            self.num_user_node = len(self.edges['user'].unique())\n",
    "            self.num_item_node = len(self.edges['item'].unique())\n",
    "            print(f\"User node : {self.num_user_node}\\n \\\n",
    "                  Item node : {self.num_item_node}\\n \\\n",
    "                  edges : {len(self.edges)}\")\n",
    "        else:\n",
    "            FileNotFoundError(f\"There are No train data in {path}\")\n",
    "\n",
    "        if User_feature_path != None:\n",
    "            if os.path.exists(User_feature_path):\n",
    "                self.user_feature = pd.read_table(User_feature_path)\n",
    "            else:\n",
    "                FileNotFoundError(f\"There are No user feature data in {User_feature_path}\")\n",
    "        else:\n",
    "            User_feature_path = None\n",
    "        if Item_feature_path != None:\n",
    "            if os.path.exists(Item_feature_path):\n",
    "                self.item_feature = pd.read_table(Item_feature_path)\n",
    "            else:\n",
    "                FileNotFoundError(f\"There are No user feature data in {Item_feature_path}\")\n",
    "        else:\n",
    "            Item_feature_path = None\n",
    "        self.get_adj_matrix()\n",
    "        return self.edges\n",
    "    \n",
    "    def get_adj_matrix(self):\n",
    "        adj_M = sp.coo_matrix((np.ones(len(self.edges)),(self.edges['user'],self.edges['item'])), dtype=np.float32)\n",
    "        self.ui_matrix = adj_M\n",
    "        self.iu_matrix = adj_M.transpose()\n",
    "\n",
    "\n",
    "# class Gowalla(object):\n",
    "#     \"\"\"\n",
    "#     To load the Gowalla data\n",
    "#     Current Version could be used as abstrct class after\n",
    "#     Gowalla only contains\n",
    "#     edge : \"user\", \"item\", \"timestamp\"\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         super(Gowalla,self).__init__()\n",
    "    \n",
    "\n",
    "#     def load_data(self, train_path, test_path, User_feature_path = None, Item_feature_path = None):\n",
    "#         \"\"\" \n",
    "#         Load the data from file path\n",
    "#         Parameter:\n",
    "#         train_path : str => file path for train data set\n",
    "#         test_path : str => file path for test data set\n",
    "#         User_feature_path : str , default = None => file path for user data \n",
    "#         Item_feature_path : str , default = None => file path for item data \n",
    "#         \"\"\"\n",
    "#         if os.path.exists(train_path):\n",
    "#             self.train_edges = pd.read_table(train_path, names=[\"user\",\"item\",\"timestamp\"], sep=\" \",dtype={'user':np.int64,'item':np.int64})\n",
    "#             self.train_num_user_node = len(self.train_edges['user'].unique())\n",
    "#             self.train_num_item_node = len(self.train_edges['item'].unique())\n",
    "#             print(f\"User node : {self.train_num_user_node}\\n \\\n",
    "#                   Item node : {self.train_num_item_node}\\n \\\n",
    "#                   edges : {len(self.train_edges)}\")\n",
    "#         else:\n",
    "#             FileNotFoundError(f\"There are No train data in {train_path}\")\n",
    "\n",
    "#         if os.path.exists(test_path):\n",
    "#             self.test_edges = pd.read_table(test_path, names=[\"user\",\"item\",\"timestamp\"], sep=\" \",dtype={'user':np.int64,'item':np.int64})\n",
    "#             self.test_num_user_node = len(self.test_edges['user'].unique())\n",
    "#             self.test_num_item_node = len(self.test_edges['item'].unique())\n",
    "#             print(f\"User node : {self.test_num_user_node}\\n \\\n",
    "#                   Item node : {self.test_num_item_node}\\n \\\n",
    "#                   edges : {len(self.test_edges)}\")\n",
    "#         else:\n",
    "#             FileNotFoundError(f\"There are No test data in {test_path}\")\n",
    "        \n",
    "#         if User_feature_path != None:\n",
    "#             if os.path.exists(User_feature_path):\n",
    "#                 self.user_feature = pd.read_table(User_feature_path)\n",
    "#             else:\n",
    "#                 FileNotFoundError(f\"There are No user feature data in {User_feature_path}\")\n",
    "#         else:\n",
    "#             User_feature_path = None\n",
    "#         if Item_feature_path != None:\n",
    "#             if os.path.exists(Item_feature_path):\n",
    "#                 self.item_feature = pd.read_table(Item_feature_path)\n",
    "#             else:\n",
    "#                 FileNotFoundError(f\"There are No user feature data in {Item_feature_path}\")\n",
    "#         else:\n",
    "#             Item_feature_path = None\n",
    "#         self.get_adj_matrix()\n",
    "#         return self.train_edges, self.test_edges \n",
    "    \n",
    "#     def get_adj_matrix(self):\n",
    "#         train_adj_M = sp.coo_matrix((np.ones(len(self.train_edges)),(self.train_edges['user'],self.train_edges['item'])), dtype=np.float32)\n",
    "#         test_adj_M = sp.coo_matrix((np.ones(len(self.train_edges)),(self.train_edges['user'],self.train_edges['item'])), dtype=np.float32)\n",
    "#         self.train_ui_matrix = train_adj_M\n",
    "#         self.train_iu_matrix = train_adj_M.transpose()\n",
    "#         self.test_ui_matrix = test_adj_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User node : 29858\n",
      "                   Item node : 40988\n",
      "                   edges : 821971\n"
     ]
    }
   ],
   "source": [
    "data = Gowalla()\n",
    "train = data.load_data('C:\\\\Users\\\\PC\\\\Desktop\\\\MGCCF\\\\data\\\\Gowalla\\\\train.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MMGCF(nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super(MMGCF, self).__init__()\n",
    "        self.n_user = data.num_user_node\n",
    "        self.n_item = data.num_item_node\n",
    "        self.emb_size = config['emb_size']\n",
    "        self.layers = config['n_layers']\n",
    "\n",
    "    def init_weight(self):\n",
    "        # xavier init\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.n_user,\n",
    "                                                 self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.n_item,\n",
    "                                                 self.emb_size)))\n",
    "        })\n",
    "        \n",
    "        #init the paramenter\n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layers\n",
    "        for k in range(len(self.layers)):\n",
    "            weight_dict.update({'W_gc_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_gc_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "            weight_dict.update({'W_ag_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_ag_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "    \n",
    "    def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_Notebook",
   "language": "python",
   "name": "jupyter_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
