{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from enum import Enum\n",
    "# from parse import parse_args\n",
    "import multiprocessing\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# args = parse_args()\n",
    "\n",
    "# ROOT_PATH = os.path.dirname(os.path.dirname(__file__))\n",
    "# CODE_PATH = join(ROOT_PATH, 'code')\n",
    "# DATA_PATH = join(ROOT_PATH, 'data')\n",
    "# BOARD_PATH = join(CODE_PATH, 'runs')\n",
    "# FILE_PATH = join(CODE_PATH, 'checkpoints')\n",
    "# import sys\n",
    "# sys.path.append(join(CODE_PATH, 'sources'))\n",
    "\n",
    "\n",
    "# if not os.path.exists(FILE_PATH):\n",
    "#     os.makedirs(FILE_PATH, exist_ok=True)\n",
    "\n",
    "config = {}\n",
    "all_dataset = ['lastfm', 'gowalla', 'yelp2018', 'amazon-book']\n",
    "all_models  = ['mf', 'lgn']\n",
    "# config['batch_size'] = 4096\n",
    "config['bpr_batch_size'] = 2048 # the batch size for bpr loss training procedure\"\n",
    "config['latent_dim_rec'] = 64 # the embedding size of lightGCN\n",
    "config['lightGCN_n_layers']= 3 # the layer num of lightGCN\n",
    "config['dropout'] = 0 # using the dropout or not\n",
    "config['keep_prob']  = 0.6 # the batch size for bpr loss training procedure\n",
    "config['A_n_fold'] = 100 # the fold num used to split large adj matrix, like gowalla\n",
    "config['test_u_batch_size'] = 100 # the batch size of users for testing\n",
    "config['multicore'] = 0 # whether we use multiprocessing or not in test\n",
    "config['lr'] = 0.001 # the learning rate\n",
    "config['decay'] = 1e-4 # the weight decay for l2 normalizaton\n",
    "config['pretrain'] = 0 # whether we use pretrained weight or not\n",
    "config['A_split'] = False #\n",
    "config['bigdata'] = False #\n",
    "\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n",
    "CORES = multiprocessing.cpu_count() // 2\n",
    "seed = 2020\n",
    "\n",
    "dataset = 'gowalla'\n",
    "model_name = 'lgn'\n",
    "if dataset not in all_dataset:\n",
    "    raise NotImplementedError(f\"Haven't supported {dataset} yet!, try {all_dataset}\")\n",
    "if model_name not in all_models:\n",
    "    raise NotImplementedError(f\"Haven't supported {model_name} yet!, try {all_models}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_epochs = 1000\n",
    "LOAD = 0\n",
    "PATH = \"./checkpoints\" # path to save weights\n",
    "topks = eval(\"[20]\") # @k test list\n",
    "tensorboard = 1 # enable tensorboard\n",
    "comment = \"lgn\"\n",
    "# let pandas shut up\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "# abstracted class for every data\n",
    "# class BasicDataset(Dataset):\n",
    "#     def __init__(self):\n",
    "#         print(\"init dataset\")\n",
    "    \n",
    "#     # @property - getter / @variable.setter - setter\n",
    "#     @property\n",
    "#     def n_users(self):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     @property\n",
    "#     def m_items(self):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     @property\n",
    "#     def trainDataSize(self):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     @property\n",
    "#     def testDict(self):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     @property\n",
    "#     def allPos(self):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     def getUserItemFeedback(self, users, items):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     def getUserPosItems(self, users):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     def getUserNegItems(self, users):\n",
    "#         \"\"\"\n",
    "#         not necessary for large dataset\n",
    "#         it's stupid to return all neg items in super large dataset\n",
    "#         \"\"\"\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     def getSparseGraph(self):\n",
    "#         \"\"\"\n",
    "#         build a graph in torch.sparse.IntTensor.\n",
    "#         Details in NGCF's matrix form\n",
    "#         A = \n",
    "#             |I,   R|\n",
    "#             |R^T, I|\n",
    "#         \"\"\"\n",
    "#         raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat(self,A):\n",
    "    A_fold = []\n",
    "    fold_len = (self.n_users + self.m_items) // self.folds\n",
    "    for i_fold in range(self.folds):\n",
    "        start = i_fold*fold_len\n",
    "        if i_fold == self.folds - 1:\n",
    "            end = self.n_users + self.m_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "        A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(world.device))\n",
    "    return A_fold\n",
    "\n",
    "def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "    coo = X.tocoo().astype(np.float32)\n",
    "    row = torch.Tensor(coo.row).long()\n",
    "    col = torch.Tensor(coo.col).long()\n",
    "    index = torch.stack([row, col])\n",
    "    data = torch.FloatTensor(coo.data)\n",
    "    return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "    \n",
    "def getSparseGraph(self):\n",
    "    print(\"loading adjacency matrix\")\n",
    "    if self.Graph is None:\n",
    "        try:\n",
    "            pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n",
    "            print(\"successfully loaded...\")\n",
    "            norm_adj = pre_adj_mat\n",
    "        except :\n",
    "            print(\"generating adjacency matrix\")\n",
    "            s = time()\n",
    "            adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
    "            adj_mat = adj_mat.tolil()\n",
    "            R = self.UserItemNet.tolil()\n",
    "            adj_mat[:self.n_users, self.n_users:] = R\n",
    "            adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "            adj_mat = adj_mat.todok()\n",
    "            # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "            \n",
    "            rowsum = np.array(adj_mat.sum(axis=1))\n",
    "            d_inv = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat = sp.diags(d_inv)\n",
    "            \n",
    "            norm_adj = d_mat.dot(adj_mat)\n",
    "            norm_adj = norm_adj.dot(d_mat)\n",
    "            norm_adj = norm_adj.tocsr()\n",
    "            end = time()\n",
    "            print(f\"costing {end-s}s, saved norm_mat...\")\n",
    "            sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "        if self.split == True:\n",
    "            self.Graph = self._split_A_hat(norm_adj)\n",
    "            print(\"done split matrix\")\n",
    "        else:\n",
    "            self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "            self.Graph = self.Graph.coalesce().to(world.device)\n",
    "            print(\"don't split the matrix\")\n",
    "    return self.Graph\n",
    "\n",
    "def __build_test(self):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        dict: {user: [items]}\n",
    "    \"\"\"\n",
    "    test_data = {}\n",
    "    for i, item in enumerate(self.testItem):\n",
    "        user = self.testUser[i]\n",
    "        if test_data.get(user):\n",
    "            test_data[user].append(item)\n",
    "        else:\n",
    "            test_data[user] = [item]\n",
    "    return test_data\n",
    "\n",
    "def getUserItemFeedback(self, users, items):\n",
    "    \"\"\"\n",
    "    users:\n",
    "        shape [-1]\n",
    "    items:\n",
    "        shape [-1]\n",
    "    return:\n",
    "        feedback [-1]\n",
    "    \"\"\"\n",
    "    # print(self.UserItemNet[users, items])\n",
    "    return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n",
    "\n",
    "def getUserPosItems(self, users):\n",
    "    posItems = []\n",
    "    for user in users:\n",
    "        posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "    return posItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/gowalla]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# class gowalla(BasicDataset):\n",
    "#     \"\"\" \n",
    "#     Dataset type for pytorch include graph information\n",
    "#     Gowalla Dataset\n",
    "    \n",
    "#     \"\"\"\n",
    "#   def __init__(self,config = world.config,path=\"../data/gowalla\"):\n",
    "path=\"../data/gowalla\"\n",
    "split = config[\"A_split\"]\n",
    "mode_dict = {'train': 0, \"test\": 1}\n",
    "mode = mode_dict['train']\n",
    "n_user = 0\n",
    "m_item = 0\n",
    "train_file = path + '/train.txt'\n",
    "test_file = path + '/test.txt'\n",
    "path = path\n",
    "trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "testUniqueUsers, testItem, testUser = [], [], []\n",
    "trainDataSize = 0\n",
    "testDataSize = 0\n",
    "\n",
    "with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    m_item = max(m_item, max(items))\n",
    "                    n_user = max(n_user, uid)\n",
    "                    trainDataSize += len(items)\n",
    "trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "trainUser = np.array(trainUser)\n",
    "trainItem = np.array(trainItem)\n",
    "\n",
    "with open(test_file) as f:\n",
    "    for l in f.readlines():\n",
    "        if len(l) > 0:\n",
    "            l = l.strip('\\n').split(' ')\n",
    "            items = [int(i) for i in l[1:]]\n",
    "            uid = int(l[0])\n",
    "            testUniqueUsers.append(uid)\n",
    "            testUser.extend([uid] * len(items))\n",
    "            testItem.extend(items)\n",
    "            m_item = max(m_item, max(items))\n",
    "            n_user = maxn_user, uid)\n",
    "            testDataSize += len(items)\n",
    "m_item += 1\n",
    "n_user += 1\n",
    "testUniqueUsers = np.array(testUniqueUsers)\n",
    "testUser = np.array(testUser)\n",
    "testItem = np.array(testItem)\n",
    "\n",
    "Graph = None\n",
    "print(f\"{trainDataSize} interactions for training\")\n",
    "print(f\"{testDataSize} interactions for testing\")\n",
    "print(f\"{dataset} Sparsity : {(trainDataSize + testDataSize) / n_user / m_item}\")\n",
    "\n",
    "# (users,items), bipartite graph\n",
    "UserItemNet = csr_matrix((np.ones(len(trainUser)), (trainUser, trainItem)),\n",
    "                                shape=(n_user, m_item))\n",
    "users_D = np.array(UserItemNet.sum(axis=1)).squeeze()\n",
    "users_D[users_D == 0.] = 1\n",
    "items_D = np.array(UserItemNet.sum(axis=0)).squeeze()\n",
    "items_D[items_D == 0.] = 1.\n",
    "# pre-calculate\n",
    "_allPos = getUserPosItems(list(range(n_user)))\n",
    "__testDict = __build_test()\n",
    "print(f\"{world.dataset} is ready to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_notebook",
   "language": "python",
   "name": "jupyter_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
