{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#cudf for linux\n",
    "# %conda install -c rapidsai -c conda-forge -c nvidia cudf=23.04 python=3.10 cudatoolkit=11.8\n",
    "#https://github.com/rapidsai/cudf\n",
    "#https://docs.rapids.ai/install#WSL2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to df\n",
    "## Load data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "#Gowalla setting\n",
    "#================================================\n",
    "DATA_NAME = \"LastFM\"\n",
    "FILE_NAME = \"user_taggedartists-timestamps.dat\"\n",
    "COL_NAMES = ['userID','artistID','tagID','timestamp']\n",
    "BASE_DATA_RATIO = 6# N/10\n",
    "INC_RATIO = 4\n",
    "INC_STEP = 4\n",
    "#================================================\n",
    "\n",
    "# config\n",
    "ROOT_PATH = os.path.abspath(\"\")\n",
    "DATA_PATH = join(ROOT_PATH, DATA_NAME)\n",
    "SAVE_PATH = join(ROOT_PATH, DATA_NAME)\n",
    "FILE_PATH = join(DATA_PATH, FILE_NAME)\n",
    "FILT_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load File\n",
    "origin_df = pd.read_table(FILE_PATH, names=COL_NAMES)\n",
    "#origin_df = cudf.from_pandas(origin_df)\n",
    "def print_info(df):\n",
    "        print(f\"Total Edges : {len(df)}\\nTotal User : {len(df['user'].unique())}\\nTotal item : {len(df['item'].unique())} \\\n",
    "                \\nSmallest user id : {df['user'].unique().min()} \\\n",
    "                \\nbiggest user id : {df['user'].unique().max()} \\\n",
    "                \\nSmallest item id : {df['item'].unique().min()} \\\n",
    "                \\nbiggest item id : {df['item'].unique().max()} \\\n",
    "                \\nMin Interaction Per user : {df.user.value_counts().min()} \\\n",
    "                \\nMax Interaction Per user : {df.user.value_counts().max()} \\\n",
    "                \\nAvg Interaction Per user : {df.user.value_counts().mean()}\\\n",
    "                \\nMin Interaction Per item : {df.item.value_counts().min()} \\\n",
    "                \\nMax Interaction Per item : {df.item.value_counts().max()} \\\n",
    "                \\nAvg Interaction Per item : {df.item.value_counts().mean()}\")\n",
    "print_info(origin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음번 부터는 필터링 하고 리팩토링 할것 필요 없는 것들 까지들어가있어서 시간이 훨씬 오래걸린다......"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반복문이 아닌 판다스 기본 기능을 활용하니까 훨씬 빠르다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refactoring_from_0(df):\n",
    "    out_df = pd.DataFrame() \n",
    "    \n",
    "    original_uid = np.sort(df['user'].unique())\n",
    "    original_iid = np.sort(df['item'].unique())\n",
    "\n",
    "    u_range = range(len(original_uid))\n",
    "    i_range = range(len(original_iid))\n",
    "\n",
    "    uid_mapping = { o_id: n_id for o_id, n_id in zip(original_uid, u_range)} # 원래 유저 아이디 (중간중간 비어있음) : 순서대로 유저 아이디\n",
    "    iid_mapping = { o_id: n_id for o_id, n_id in zip(original_iid,i_range)} # 원래 아이템 아이디 : 순서대로 아이템 아이디\n",
    "\n",
    "    uid_map = pd.DataFrame({'o_id' : list(uid_mapping.keys()), 'n_id' : list(uid_mapping.values())})\n",
    "\n",
    "    iid_map = pd.DataFrame({'o_id' : list(iid_mapping.keys()),'n_id':list(iid_mapping.values())})\n",
    "\n",
    "\n",
    "    out_df['user'] = df['user'].map(uid_mapping)\n",
    "    out_df['item'] = df['item'].map(iid_mapping)\n",
    "    out_df['time'] = df['time']\n",
    "    return out_df, uid_map, iid_map\n",
    "\n",
    "\n",
    "# NUM_CORES = 8\n",
    "\n",
    "# def user_replace(df):\n",
    "#     return df.replace({'user':{user_id:id}})\n",
    "\n",
    "# def item_replace(df):\n",
    "#     return df.replace({'item':{item_id:id}})\n",
    "\n",
    "\n",
    "# def parallelize_dataframe(df, func):\n",
    "#     df_split = np.array_split(df, NUM_CORES)\n",
    "#     pool = Pool(NUM_CORES)\n",
    "#     df = pd.concat(pool.map(func, df_split))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return df\n",
    "\n",
    "# Change each user and item index start from 0\n",
    "# origin_df.sort_values(by='user',ascending=True)\n",
    "\n",
    "# # Change user id to start from 0\n",
    "# id = 0\n",
    "\n",
    "\n",
    "# for user_id in tqdm(origin_df['user'].unique()):\n",
    "#     parameter = {'user':{user_id:id}}\n",
    "#     # origin_df = parallelize_dataframe(origin_df, user_replace)\n",
    "#     id += 1\n",
    "\n",
    "# Change item id to start from 0\n",
    "# origin_df.sort_values(by='item',ascending=True)\n",
    "# id = 0\n",
    "\n",
    "# for item_id in tqdm(origin_df['item'].unique()):\n",
    "#     parameter = {'item':{item_id:id}}\n",
    "#     # origin_df = parallelize_dataframe(origin_df, item_replace)\n",
    "#     id += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Test refactoring function \n",
    "temp_df, a, b = refactoring_from_0(origin_df)\n",
    "print_info(temp_df)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Save refactored dataframe\n",
    "temp_df.to_csv(DATA_PATH+\"_refactored.intr\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\PC\\\\Desktop\\\\MGCCF\\\\data\\\\Gowalla_refactored.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[102], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Load refactored dataframe\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m origin_df \u001B[39m=\u001B[39m pd\u001B[39m.\u001B[39;49mread_table(DATA_PATH\u001B[39m+\u001B[39;49m\u001B[39m\"\u001B[39;49m\u001B[39m_refactored.txt\u001B[39;49m\u001B[39m\"\u001B[39;49m, names\u001B[39m=\u001B[39;49mCOL_NAMES)\n",
      "File \u001B[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[39m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[39mreturn\u001B[39;00m func(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(args) \u001B[39m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[39m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[39m.\u001B[39mformat(arguments\u001B[39m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[39mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[39m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[39mreturn\u001B[39;00m func(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1289\u001B[0m, in \u001B[0;36mread_table\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m   1274\u001B[0m kwds_defaults \u001B[39m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1275\u001B[0m     dialect,\n\u001B[0;32m   1276\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1285\u001B[0m     defaults\u001B[39m=\u001B[39m{\u001B[39m\"\u001B[39m\u001B[39mdelimiter\u001B[39m\u001B[39m\"\u001B[39m: \u001B[39m\"\u001B[39m\u001B[39m\\t\u001B[39;00m\u001B[39m\"\u001B[39m},\n\u001B[0;32m   1286\u001B[0m )\n\u001B[0;32m   1287\u001B[0m kwds\u001B[39m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1289\u001B[0m \u001B[39mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    602\u001B[0m _validate_names(kwds\u001B[39m.\u001B[39mget(\u001B[39m\"\u001B[39m\u001B[39mnames\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39mNone\u001B[39;00m))\n\u001B[0;32m    604\u001B[0m \u001B[39m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 605\u001B[0m parser \u001B[39m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwds)\n\u001B[0;32m    607\u001B[0m \u001B[39mif\u001B[39;00m chunksize \u001B[39mor\u001B[39;00m iterator:\n\u001B[0;32m    608\u001B[0m     \u001B[39mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1439\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39moptions[\u001B[39m\"\u001B[39m\u001B[39mhas_index_names\u001B[39m\u001B[39m\"\u001B[39m] \u001B[39m=\u001B[39m kwds[\u001B[39m\"\u001B[39m\u001B[39mhas_index_names\u001B[39m\u001B[39m\"\u001B[39m]\n\u001B[0;32m   1441\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mhandles: IOHandles \u001B[39m|\u001B[39m \u001B[39mNone\u001B[39;00m \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[1;32m-> 1442\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_engine \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_make_engine(f, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mengine)\n",
      "File \u001B[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1733\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39mb\u001B[39m\u001B[39m\"\u001B[39m \u001B[39mnot\u001B[39;00m \u001B[39min\u001B[39;00m mode:\n\u001B[0;32m   1734\u001B[0m         mode \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mb\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m-> 1735\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mhandles \u001B[39m=\u001B[39m get_handle(\n\u001B[0;32m   1736\u001B[0m     f,\n\u001B[0;32m   1737\u001B[0m     mode,\n\u001B[0;32m   1738\u001B[0m     encoding\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49moptions\u001B[39m.\u001B[39;49mget(\u001B[39m\"\u001B[39;49m\u001B[39mencoding\u001B[39;49m\u001B[39m\"\u001B[39;49m, \u001B[39mNone\u001B[39;49;00m),\n\u001B[0;32m   1739\u001B[0m     compression\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49moptions\u001B[39m.\u001B[39;49mget(\u001B[39m\"\u001B[39;49m\u001B[39mcompression\u001B[39;49m\u001B[39m\"\u001B[39;49m, \u001B[39mNone\u001B[39;49;00m),\n\u001B[0;32m   1740\u001B[0m     memory_map\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49moptions\u001B[39m.\u001B[39;49mget(\u001B[39m\"\u001B[39;49m\u001B[39mmemory_map\u001B[39;49m\u001B[39m\"\u001B[39;49m, \u001B[39mFalse\u001B[39;49;00m),\n\u001B[0;32m   1741\u001B[0m     is_text\u001B[39m=\u001B[39;49mis_text,\n\u001B[0;32m   1742\u001B[0m     errors\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49moptions\u001B[39m.\u001B[39;49mget(\u001B[39m\"\u001B[39;49m\u001B[39mencoding_errors\u001B[39;49m\u001B[39m\"\u001B[39;49m, \u001B[39m\"\u001B[39;49m\u001B[39mstrict\u001B[39;49m\u001B[39m\"\u001B[39;49m),\n\u001B[0;32m   1743\u001B[0m     storage_options\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49moptions\u001B[39m.\u001B[39;49mget(\u001B[39m\"\u001B[39;49m\u001B[39mstorage_options\u001B[39;49m\u001B[39m\"\u001B[39;49m, \u001B[39mNone\u001B[39;49;00m),\n\u001B[0;32m   1744\u001B[0m )\n\u001B[0;32m   1745\u001B[0m \u001B[39massert\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mhandles \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m f \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mhandles\u001B[39m.\u001B[39mhandle\n",
      "File \u001B[1;32mc:\\Users\\PC\\.conda\\envs\\Jupyter_Notebook\\lib\\site-packages\\pandas\\io\\common.py:856\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    851\u001B[0m \u001B[39melif\u001B[39;00m \u001B[39misinstance\u001B[39m(handle, \u001B[39mstr\u001B[39m):\n\u001B[0;32m    852\u001B[0m     \u001B[39m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    853\u001B[0m     \u001B[39m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[39mif\u001B[39;00m ioargs\u001B[39m.\u001B[39mencoding \u001B[39mand\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39mb\u001B[39m\u001B[39m\"\u001B[39m \u001B[39mnot\u001B[39;00m \u001B[39min\u001B[39;00m ioargs\u001B[39m.\u001B[39mmode:\n\u001B[0;32m    855\u001B[0m         \u001B[39m# Encoding\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m         handle \u001B[39m=\u001B[39m \u001B[39mopen\u001B[39;49m(\n\u001B[0;32m    857\u001B[0m             handle,\n\u001B[0;32m    858\u001B[0m             ioargs\u001B[39m.\u001B[39;49mmode,\n\u001B[0;32m    859\u001B[0m             encoding\u001B[39m=\u001B[39;49mioargs\u001B[39m.\u001B[39;49mencoding,\n\u001B[0;32m    860\u001B[0m             errors\u001B[39m=\u001B[39;49merrors,\n\u001B[0;32m    861\u001B[0m             newline\u001B[39m=\u001B[39;49m\u001B[39m\"\u001B[39;49m\u001B[39m\"\u001B[39;49m,\n\u001B[0;32m    862\u001B[0m         )\n\u001B[0;32m    863\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[39m# Binary mode\u001B[39;00m\n\u001B[0;32m    865\u001B[0m         handle \u001B[39m=\u001B[39m \u001B[39mopen\u001B[39m(handle, ioargs\u001B[39m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\PC\\\\Desktop\\\\MGCCF\\\\data\\\\Gowalla_refactored.txt'"
     ]
    }
   ],
   "source": [
    "# Load refactored dataframe\n",
    "origin_df = pd.read_table(DATA_PATH+\"_refactored.intr\", names=COL_NAMES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "1. remove duplcaite\n",
    "2. filter data which have under 10 interaction\n",
    "3. refactor the id of user and item start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = origin_df\n",
    "# Just interaction and drop duplicate\n",
    "df = df[['user','time','item']]\n",
    "df = df.drop_duplicates(subset=['user','item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Interaction information without filtering\n",
    "WO_filtering = df\n",
    "# Sort with ascending order with user first and item\n",
    "WO_filtering = WO_filtering.sort_values(by=['user','item','time'], ascending=[True,True,True])\n",
    "WO_filtering, a, b = refactoring_from_0(WO_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WO_filtering_filename = DATA_PATH + \"_WO_Filtering.intr\"\n",
    "if not os.path.exists(WO_filtering_filename):\n",
    "    WO_filtering.to_csv(WO_filtering_filename,sep=\" \", index=False, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data that has less than 10 interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Edges : 1027464\n",
      "Total User : 29858\n",
      "Total item : 40988                 \n",
      "Smallest user id : 0                 \n",
      "biggest user id : 196183                 \n",
      "Smallest item id : 8932                 \n",
      "biggest item id : 5838873                 \n",
      "Min Interaction Per user : 10                 \n",
      "Max Interaction Per user : 1014                 \n",
      "Avg Interaction Per user : 34.411681961283406                \n",
      "Min Interaction Per item : 10                 \n",
      "Max Interaction Per item : 2310                 \n",
      "Avg Interaction Per item : 25.067434371035425\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>time</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-19T23:55:27Z</td>\n",
       "      <td>22847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-18T22:17:43Z</td>\n",
       "      <td>420315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-17T23:42:03Z</td>\n",
       "      <td>316637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-17T19:26:05Z</td>\n",
       "      <td>16516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16T18:50:42Z</td>\n",
       "      <td>5535878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027459</th>\n",
       "      <td>196183</td>\n",
       "      <td>2010-04-27T04:32:19Z</td>\n",
       "      <td>73521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027460</th>\n",
       "      <td>196183</td>\n",
       "      <td>2010-03-14T18:13:45Z</td>\n",
       "      <td>9724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027461</th>\n",
       "      <td>196183</td>\n",
       "      <td>2010-03-13T03:29:55Z</td>\n",
       "      <td>388127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027462</th>\n",
       "      <td>196183</td>\n",
       "      <td>2010-02-15T03:33:42Z</td>\n",
       "      <td>55858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027463</th>\n",
       "      <td>196183</td>\n",
       "      <td>2010-02-13T07:49:03Z</td>\n",
       "      <td>197557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1027464 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user                  time     item\n",
       "0             0  2010-10-19T23:55:27Z    22847\n",
       "1             0  2010-10-18T22:17:43Z   420315\n",
       "2             0  2010-10-17T23:42:03Z   316637\n",
       "3             0  2010-10-17T19:26:05Z    16516\n",
       "4             0  2010-10-16T18:50:42Z  5535878\n",
       "...         ...                   ...      ...\n",
       "1027459  196183  2010-04-27T04:32:19Z    73521\n",
       "1027460  196183  2010-03-14T18:13:45Z     9724\n",
       "1027461  196183  2010-03-13T03:29:55Z   388127\n",
       "1027462  196183  2010-02-15T03:33:42Z    55858\n",
       "1027463  196183  2010-02-13T07:49:03Z   197557\n",
       "\n",
       "[1027464 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering_filename = DATA_PATH + \"_Filtering_refactored.intr\"\n",
    "fdf = df\n",
    "\n",
    "while fdf.user.value_counts().min() < FILT_THRESHOLD or fdf.item.value_counts().min() < FILT_THRESHOLD:\n",
    "    df_item = fdf.groupby('item').count()\n",
    "    df_item = df_item[df_item.user < FILT_THRESHOLD]\n",
    "    li = df_item.index.to_list()\n",
    "    fdf = fdf.drop(fdf.loc[fdf.item.isin(li)].index)\n",
    "    # print_info(fdf)\n",
    "    df_usr = fdf.groupby('user').count()\n",
    "    df_usr = df_usr[df_usr.item < FILT_THRESHOLD]\n",
    "    li = df_usr.index.to_list()\n",
    "    fdf = fdf.drop(fdf.loc[fdf.user.isin(li)].index)\n",
    "    # print_info(fdf)\n",
    "    # print(f\"Total Edges : {len(fdf)}\\nTotal User : {len(fdf['user'].unique())}\\nTotal item : {len(fdf['item'].unique())} \\\n",
    "    #             \\nMin Interaction Per user : {fdf.user.value_counts().min()} \\\n",
    "    #             \\nMax Interaction Per user : {fdf.user.value_counts().max()} \\\n",
    "    #             \\nAvg Interaction Per user : {fdf.user.value_counts().mean()}\\\n",
    "    #             \\nMin Interaction Per item : {fdf.item.value_counts().min()} \\\n",
    "    #             \\nMax Interaction Per item : {fdf.item.value_counts().max()} \\\n",
    "    #             \\nAvg Interaction Per item : {fdf.item.value_counts().mean()}\")\n",
    "print_info(fdf)\n",
    "fdf = fdf.reset_index().drop(columns = ['index'])\n",
    "fdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor to make each user and item number start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Edges : 1027464\n",
      "Total User : 29858\n",
      "Total item : 40988                 \n",
      "Smallest user id : 0                 \n",
      "biggest user id : 29857                 \n",
      "Smallest item id : 0                 \n",
      "biggest item id : 40987                 \n",
      "Min Interaction Per user : 10                 \n",
      "Max Interaction Per user : 1014                 \n",
      "Avg Interaction Per user : 34.411681961283406                \n",
      "Min Interaction Per item : 10                 \n",
      "Max Interaction Per item : 2310                 \n",
      "Avg Interaction Per item : 25.067434371035425\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4169</td>\n",
       "      <td>2010-10-19T23:55:27Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31533</td>\n",
       "      <td>2010-10-18T22:17:43Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>29074</td>\n",
       "      <td>2010-10-17T23:42:03Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2572</td>\n",
       "      <td>2010-10-17T19:26:05Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40979</td>\n",
       "      <td>2010-10-16T18:50:42Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027459</th>\n",
       "      <td>29857</td>\n",
       "      <td>13173</td>\n",
       "      <td>2010-04-27T04:32:19Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027460</th>\n",
       "      <td>29857</td>\n",
       "      <td>392</td>\n",
       "      <td>2010-03-14T18:13:45Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027461</th>\n",
       "      <td>29857</td>\n",
       "      <td>30837</td>\n",
       "      <td>2010-03-13T03:29:55Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027462</th>\n",
       "      <td>29857</td>\n",
       "      <td>10578</td>\n",
       "      <td>2010-02-15T03:33:42Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027463</th>\n",
       "      <td>29857</td>\n",
       "      <td>24374</td>\n",
       "      <td>2010-02-13T07:49:03Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1027464 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user   item                  time\n",
       "0            0   4169  2010-10-19T23:55:27Z\n",
       "1            0  31533  2010-10-18T22:17:43Z\n",
       "2            0  29074  2010-10-17T23:42:03Z\n",
       "3            0   2572  2010-10-17T19:26:05Z\n",
       "4            0  40979  2010-10-16T18:50:42Z\n",
       "...        ...    ...                   ...\n",
       "1027459  29857  13173  2010-04-27T04:32:19Z\n",
       "1027460  29857    392  2010-03-14T18:13:45Z\n",
       "1027461  29857  30837  2010-03-13T03:29:55Z\n",
       "1027462  29857  10578  2010-02-15T03:33:42Z\n",
       "1027463  29857  24374  2010-02-13T07:49:03Z\n",
       "\n",
       "[1027464 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf, filtered_uid_map, filtered_iid_map = refactoring_from_0(fdf)\n",
    "print_info(fdf)\n",
    "fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.to_csv(filtering_filename,sep=\" \", index=False, header=None)\n",
    "filtered_uid_map.to_csv(DATA_PATH+\"_filtered_uid_map.uid\",sep=\" \", index=False, header=None)\n",
    "filtered_iid_map.to_csv(DATA_PATH+\"_filtered_iid_map.iid\",sep=\" \", index=False, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item Time Sequential Incremental Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027464\n",
      "821971\n",
      "Inc0 Block Size : 41098\n",
      "Inc1 Block Size : 41098\n",
      "Inc2 Block Size : 41098\n",
      "Inc3 Block Size : 41098\n",
      "Inc4 Block Size : 41101\n"
     ]
    }
   ],
   "source": [
    "INC_TIME_FILE_PATH = os.path.join(SAVE_PATH,f\"Incremental\\\\{DATA_NAME}\\\\Time\\\\\")\n",
    "fdf = fdf.sort_values(by=\"user\", ascending=True)\n",
    "print(len(fdf))\n",
    "pivot = len(fdf)*8//10\n",
    "fdf[:pivot].to_csv(INC_TIME_FILE_PATH + \"base.data\", sep=\" \", header=None, index=False)\n",
    "print(pivot)\n",
    "remain = fdf[pivot:]\n",
    "start = 0\n",
    "length = (len(fdf) - pivot)//5\n",
    "inc_block = []\n",
    "for i in range(INC_STEP):\n",
    "    if i != INC_STEP-1:\n",
    "        inc_block.append(remain[start:start+length])\n",
    "    else:\n",
    "        inc_block.append(remain[start:])\n",
    "    start += length\n",
    "    print(f\"Inc{i} Block Size : {len(inc_block[i])}\")\n",
    "    inc_block[i].to_csv(INC_TIME_FILE_PATH+f\"inc{i+1}.data\", sep = \" \", header=None, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total User = 29858\n",
      "Base Block Size : 822003\n",
      "Base Done\n",
      "Inc1 Done\n",
      "Inc Block1 Size : 41111\n",
      "Inc2 Done\n",
      "Inc Block2 Size : 41119\n",
      "Inc3 Done\n",
      "Inc Block3 Size : 41122\n",
      "Inc4 Done\n",
      "Inc Block4 Size : 41131\n",
      "Inc5 Done\n",
      "Inc Block5 Size : 40978\n"
     ]
    }
   ],
   "source": [
    "INC_USER_FILE_PATH = os.path.join(SAVE_PATH,f\"Incremental\\\\{DATA_NAME}\\\\User\\\\\")\n",
    "print(f\"Total User = {len(fdf['user'].unique())}\")\n",
    "# Make df containing number of user interaction\n",
    "ucdf = fdf.groupby('user').count()\n",
    "# Shuffle the df\n",
    "ucdf = ucdf.iloc[np.random.permutation(ucdf.index)].reset_index()\n",
    "\n",
    "is_base = True\n",
    "\n",
    "base_user = []\n",
    "inc_user = [[] for i in range(INC_STEP)]\n",
    "\n",
    "i = 0\n",
    "cur = 0\n",
    "\n",
    "while i < INC_STEP:\n",
    "    count = 0\n",
    "    if is_base:\n",
    "        # Base block\n",
    "        while count < 821971:\n",
    "            row = ucdf.iloc[cur]\n",
    "            base_user.append(row['user'])\n",
    "            count += row['item']\n",
    "            cur += 1\n",
    "        print(f\"Base Block Size : {count}\")\n",
    "        with open(INC_USER_FILE_PATH+\"base_user.data\", 'w') as fp:\n",
    "            for user in base_user:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % user)\n",
    "        print('Base Done')\n",
    "        is_base = False\n",
    "    elif i == INC_STEP-1:\n",
    "        # Last incremental block\n",
    "        inc_user[i]= ucdf[cur:]['user'].values.tolist()\n",
    "        count = ucdf[cur:]['item'].values.sum()\n",
    "        with open(INC_USER_FILE_PATH+f\"inc_user_{i+1}.data\", 'w') as fp:\n",
    "            for user in inc_user[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % user)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1\n",
    "    else:\n",
    "        # Incremental block\n",
    "        while count <= 41098:\n",
    "            row = ucdf.iloc[cur]\n",
    "            inc_user[i].append(row['user'])\n",
    "            count += row['item']\n",
    "            cur += 1\n",
    "        with open(INC_USER_FILE_PATH+f\"inc_user_{i+1}.data\", 'w') as fp:\n",
    "            for user in inc_user[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % user)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Item = 40988\n",
      "Base Block Size : 821989\n",
      "Base Done\n",
      "Inc1 Done\n",
      "Inc Block1 Size : 41273\n",
      "Inc2 Done\n",
      "Inc Block2 Size : 41161\n",
      "Inc3 Done\n",
      "Inc Block3 Size : 41112\n",
      "Inc4 Done\n",
      "Inc Block4 Size : 41107\n",
      "Inc5 Done\n",
      "Inc Block5 Size : 40822\n"
     ]
    }
   ],
   "source": [
    "INC_ITEM_FILE_PATH = os.path.join(SAVE_PATH,f\"Incremental\\\\{DATA_NAME}\\\\Item\\\\\")\n",
    "print(f\"Total Item = {len(fdf['item'].unique())}\")\n",
    "# Make df containing number of user interaction\n",
    "icdf = fdf.groupby('item').count()\n",
    "# Shuffle the df\n",
    "icdf = icdf.iloc[np.random.permutation(icdf.index)].reset_index()\n",
    "\n",
    "is_base = True\n",
    "\n",
    "base_item = []\n",
    "inc_item = [[] for i in range(INC_STEP)]\n",
    "\n",
    "i = 0\n",
    "cur = 0\n",
    "\n",
    "while i < INC_STEP:\n",
    "    count = 0\n",
    "    if is_base:\n",
    "        # Base block\n",
    "        while count < 821971:\n",
    "            row = icdf.iloc[cur]\n",
    "            base_item.append(row['item'])\n",
    "            count += row['user']\n",
    "            cur += 1\n",
    "        print(f\"Base Block Size : {count}\")\n",
    "        with open(INC_ITEM_FILE_PATH+\"base_item.data\", 'w') as fp:\n",
    "            for item in base_user:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "        print('Base Done')\n",
    "        is_base = False\n",
    "    elif i == INC_STEP-1:\n",
    "        # Last incremental block\n",
    "        inc_item[i]= icdf[cur:]['item'].values.tolist()\n",
    "        count = icdf[cur:]['user'].values.sum()\n",
    "        with open(INC_ITEM_FILE_PATH+f\"inc_item_{i+1}.data\", 'w') as fp:\n",
    "            for item in inc_item[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1\n",
    "    else:\n",
    "        # Incremental block\n",
    "        while count <= 41098:\n",
    "            row = icdf.iloc[cur]\n",
    "            inc_item[i].append(row['item'])\n",
    "            count += row['user']\n",
    "            cur += 1\n",
    "        with open(INC_ITEM_FILE_PATH+f\"inc_item_{i+1}.data\", 'w') as fp:\n",
    "            for item in inc_item[i]:\n",
    "                # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "        print(f'Inc{i+1} Done')\n",
    "        print(f\"Inc Block{i+1} Size : {count}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter_notebook",
   "language": "python",
   "name": "jupyter_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
