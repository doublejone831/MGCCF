{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "class NCF_EWC(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=8):\n",
    "        super(NCF_EWC, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(emb_size * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.ewc_lambda = 0.5  # ewc 중요도 계수, hyper param\n",
    "        self.prev_params = {}  # 이전 학습에서 계산된 중요도 정보를 저장\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_emb = self.user_emb(user_indices)\n",
    "        item_emb = self.item_emb(item_indices)\n",
    "        concat = torch.cat((user_emb, item_emb), dim=1)\n",
    "        output = self.fc_layers(concat)\n",
    "        print(f\"output: {output}\")\n",
    "        return output.squeeze()\n",
    "\n",
    "    def update_ewc_penalty(self, criterion, prev_params):\n",
    "        ewc_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in prev_params:\n",
    "                ewc_loss += torch.sum((param - prev_params[name]) ** 2) * self.ewc_lambda\n",
    "        total_loss = criterion + ewc_loss\n",
    "        return total_loss\n",
    "\n",
    "    def train_model(self, train_loader, lr=0.001, epochs=10):\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i, data in enumerate(train_loader):\n",
    "                user_indices, item_indices, ratings = data\n",
    "                user_indices = Tensor(user_indices)\n",
    "                item_indices = Tensor(item_indices)\n",
    "                ratings = Tensor(ratings.float())\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(user_indices, item_indices)\n",
    "                loss = criterion(outputs, ratings)\n",
    "                if epoch > 0:\n",
    "                    loss = self.update_ewc_penalty(loss, self.prev_params)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            self.prev_params = {name: param.detach() for name, param in self.named_parameters()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 무비렌즈"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 데이터 준비\n",
    "\n",
    "일단 태스크별로 단순히 8:2로 쪼갠다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "dfs = []\n",
    "for i in range(5):\n",
    "    dfs.append(pd.read_csv(f\"./dataset/Movielens/increase/ml_100k_inc{i}.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1478\n",
      "1446\n",
      "1478\n",
      "1469\n",
      "1451\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    print(df['item'].max())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\"\"\"\n",
    "필요한 컬럼은 유저, 아이템, rating\n",
    "\n",
    "우선 간단하게 빨리 해보는게 중요하니,\n",
    "rating이 5점이면 rating 컬럼을 1\n",
    "아니라면 0로 바꾸자고.\n",
    "\"\"\"\n",
    "cols = ['user', 'item', 'rating']\n",
    "for df in dfs:\n",
    "    df['rating'] = df['rating'].apply(lambda x: 1 if x >= 5 else 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# train/test 8:2 분리\n",
    "\"\"\"\n",
    "이것도 사실, 조금 찜찜하지만 우선 랜덤 8:2\n",
    "\"\"\"\n",
    "random_seed = 42\n",
    "\n",
    "train0, test0 = train_test_split(dfs[0], test_size=0.2, random_state=random_seed)\n",
    "train1, test1 = train_test_split(dfs[1], test_size=0.2, random_state=random_seed)\n",
    "train2, test2 = train_test_split(dfs[2], test_size=0.2, random_state=random_seed)\n",
    "train3, test3 = train_test_split(dfs[3], test_size=0.2, random_state=random_seed)\n",
    "train4, test4 = train_test_split(dfs[4], test_size=0.2, random_state=random_seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dataloader 정의"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "class MovielensDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.df.iloc[idx]['user']\n",
    "        item = self.df.iloc[idx]['item']\n",
    "        rating = self.df.iloc[idx]['rating']\n",
    "        return user, item, rating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 정의"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "# NCF 모델\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, emb_size=8, hidden_size=64):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, emb_size)\n",
    "        self.movie_embedding = nn.Embedding(n_movies, emb_size)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(emb_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, user_input, movie_input):\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        movie_embedded = self.movie_embedding(movie_input)\n",
    "        input_concat = torch.cat([user_embedded, movie_embedded], dim=-1)\n",
    "        prediction = self.fc_layers(input_concat)\n",
    "        return prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu 설정\n",
    "use_cuda = True\n",
    "\n",
    "use_cuda = use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 train/test 함수 정의"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    for user, item, rating in train_loader:\n",
    "        user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user, item).squeeze()\n",
    "        loss = criterion(output, rating.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    # print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, train_loss))\n",
    "\n",
    "    return train_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "def recall_at_k(output, target, k):\n",
    "    _, idx = torch.topk(output, k=k)\n",
    "    hit = torch.sum(target[idx])\n",
    "    return hit.float() / target.sum().float() if target.sum().float() else torch.Tensor([0])\n",
    "\n",
    "def test(model, device, test_loader, k=20):\n",
    "    model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    test_loss = 0\n",
    "    test_recall = 0\n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in test_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            output = model(user, item).squeeze()\n",
    "            loss = criterion(output, rating.float())\n",
    "            test_loss += loss.item()\n",
    "            test_recall += recall_at_k(output, rating, k).item() # recall@20 기준\n",
    "    test_loss /= len(test_loader)\n",
    "    test_recall /= len(test_loader)\n",
    "\n",
    "    return test_loss, test_recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 모델 학습\n",
    "\n",
    "1. Naive\n",
    "2. EWC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Naive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# 모델 객체 생성\n",
    "n_users = train0['user'].max()+1\n",
    "n_movies = train0['item'].max()+1\n",
    "model = NCF(n_users, n_movies).to(device)\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "train_dataset0 = MovielensDataset(train0)\n",
    "test_dataset0 = MovielensDataset(test0)\n",
    "\n",
    "train_loader0 = DataLoader(train_dataset0, batch_size=64, shuffle=True)\n",
    "test_loader0 = DataLoader(test_dataset0, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:20<00:00,  6.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "epoch = 3\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    train(model, device, train_loader0, optimizer, e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5051735848696395\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "loss, recall20 = test(model, device, test_loader0)\n",
    "print(recall20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "우선 모든 데이터에 대해 incremental training을 하고 test해보자"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "EPOCH = 50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** Train Start At TASK0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:32<00:00,  6.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 0 TASK recall20 = 0.4234914415812289\n",
      "\n",
      "************** Train Start At TASK1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 1 TASK recall20 = 0.4219339774235299\n",
      "\n",
      "************** Train Start At TASK2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 2 TASK recall20 = 0.3692853047365421\n",
      "\n",
      "************** Train Start At TASK3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:36<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 3 TASK recall20 = 0.39202805275195524\n",
      "\n",
      "avg recall : 0.40168469412331403\n"
     ]
    }
   ],
   "source": [
    "recall_list = []\n",
    "\n",
    "for i in range(len(dfs)-1):\n",
    "\n",
    "    if i == 0:\n",
    "        # base block train-test\n",
    "\n",
    "        # 데이터 준비\n",
    "        train0, test0 = train_test_split(dfs[0], test_size=0.2, shuffle=False)\n",
    "        train_dataset0 = MovielensDataset(train0)\n",
    "        test_dataset0 = MovielensDataset(test0)\n",
    "        train_loader0 = DataLoader(train_dataset0, batch_size=64, shuffle=False)\n",
    "        test_loader0 = DataLoader(test_dataset0, batch_size=64, shuffle=False)\n",
    "\n",
    "        # 모델 객체 생성\n",
    "        n_users = train0['user'].max()+1\n",
    "        n_movies = train0['item'].max()+1\n",
    "        model = NCF(n_users, n_movies).to(device)\n",
    "        # 옵티마이저 설정\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # train\n",
    "        epoch = EPOCH\n",
    "        print(f\"************** Train Start At TASK{i}\")\n",
    "        for e in tqdm(range(1, epoch+1)):\n",
    "            train(model, device, train_loader0, optimizer, e)\n",
    "\n",
    "        # test\n",
    "        _, recall20 = test(model, device, test_loader0)\n",
    "        recall_list.append(recall20)\n",
    "        print(f\"******* At {i} TASK recall20 = {recall20}\\n\")\n",
    "\n",
    "    else:\n",
    "        # inc block train-test\n",
    "\n",
    "        # 데이터 준비\n",
    "        train_dataset = MovielensDataset(dfs[i])\n",
    "        test_dataset = MovielensDataset(dfs[i+1]) # inc 블록에서는 다음 inc 블록을 test로 사용\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # train\n",
    "        epoch = EPOCH\n",
    "        print(f\"************** Train Start At TASK{i}\")\n",
    "        for e in tqdm(range(1, epoch+1)):\n",
    "            train(model, device, train_loader, optimizer, e)\n",
    "\n",
    "        # test\n",
    "        _, recall20 = test(model, device, test_loader)\n",
    "        recall_list.append(recall20)\n",
    "        print(f\"******* At {i} TASK recall20 = {recall20}\\n\")\n",
    "\n",
    "print(f\"avg recall : {sum(recall_list)/len(recall_list)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. EWC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# 모델 객체 생성\n",
    "n_users = train0['user'].max()+1\n",
    "n_movies = train0['item'].max()+1\n",
    "model = NCF(n_users, n_movies).to(device)\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "# EWC에 필요한 변수\n",
    "fisher_dict = {}\n",
    "optpar_dict = {}\n",
    "ewc_lambda = 0.4 # ewc 강도 조절.. 높을수록 이전 파라미터의 중요도가 높아짐"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# Task가 끝날 때 마다 optpar와 fisher를 저장해주는 함수.\n",
    "def on_task_update(model, device, train_loader, optimizer, task_id):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # accumulating gradients\n",
    "    for user, item, rating in train_loader:\n",
    "        user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "        output = model(user, item).squeeze()\n",
    "        loss = criterion(output, rating.float())\n",
    "        loss.backward()\n",
    "\n",
    "    fisher_dict[task_id] = {}\n",
    "    optpar_dict[task_id] = {}\n",
    "\n",
    "    # gradients accumulated can be used to calculate fisher\n",
    "    for name, param in model.named_parameters():\n",
    "        fisher_dict[task_id][name] = param.grad.data.clone().pow(2) # 누적 grad 값\n",
    "        optpar_dict[task_id][name] = param.data.clone() # 최적 grad 값"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "# EWC를 적용한 train 함수\n",
    "def train_ewc(model, device, train_loader, optimizer, epoch, task_id):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    for user, item, rating in train_loader:\n",
    "        user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user, item).squeeze()\n",
    "        loss = criterion(output, rating.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # EWC 적용 부분\n",
    "        for task in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fisher_dict[task][name]\n",
    "                optpar = optpar_dict[task][name]\n",
    "                train_loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    # print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, train_loss))\n",
    "\n",
    "    return train_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** Train Start At TASK0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:21<00:00,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 0 TASK recall20 = 0.424965131054209\n",
      "\n",
      "************** Train Start At TASK1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:40<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 1 TASK recall20 = 0.4120386938907598\n",
      "\n",
      "************** Train Start At TASK2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:42<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 2 TASK recall20 = 0.38005163246079493\n",
      "\n",
      "************** Train Start At TASK3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* At 3 TASK recall20 = 0.3987114958857235\n",
      "\n",
      "avg recall : 0.40394173832287184\n"
     ]
    }
   ],
   "source": [
    "recall_list = []\n",
    "for i in range(len(dfs)-1):\n",
    "    if i == 0:\n",
    "        # base block train-test\n",
    "\n",
    "        # 데이터 준비\n",
    "        train0, test0 = train_test_split(dfs[0], test_size=0.2, shuffle=False)\n",
    "        train_dataset0 = MovielensDataset(train0)\n",
    "        test_dataset0 = MovielensDataset(test0)\n",
    "        train_loader0 = DataLoader(train_dataset0, batch_size=64, shuffle=False)\n",
    "        test_loader0 = DataLoader(test_dataset0, batch_size=64, shuffle=False)\n",
    "\n",
    "        # 모델 객체 생성\n",
    "        n_users = train0['user'].max()+1\n",
    "        n_movies = train0['item'].max()+1\n",
    "        model = NCF(n_users, n_movies).to(device)\n",
    "        # 옵티마이저 설정\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # train\n",
    "        epoch = EPOCH\n",
    "        print(f\"************** Train Start At TASK{i}\")\n",
    "        for e in tqdm(range(1, epoch+1)):\n",
    "            train_ewc(model, device, train_loader0, optimizer, e, i)\n",
    "        on_task_update(model, device, train_loader0, optimizer, i)\n",
    "\n",
    "        # test\n",
    "        _, recall20 = test(model, device, test_loader0)\n",
    "        recall_list.append(recall20)\n",
    "        print(f\"******* At {i} TASK recall20 = {recall20}\\n\")\n",
    "\n",
    "    else:\n",
    "        # inc block train-test\n",
    "\n",
    "        # 데이터 준비\n",
    "        train_dataset = MovielensDataset(dfs[i])\n",
    "        test_dataset = MovielensDataset(dfs[i+1]) # inc 블록에서는 다음 inc 블록을 test로 사용\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # train\n",
    "        epoch = EPOCH\n",
    "        print(f\"************** Train Start At TASK{i}\")\n",
    "        for e in tqdm(range(1, epoch+1)):\n",
    "            train_ewc(model, device, train_loader, optimizer, e, i)\n",
    "        on_task_update(model, device, train_loader, optimizer, i)\n",
    "\n",
    "        # test\n",
    "        _, recall20 = test(model, device, test_loader)\n",
    "        recall_list.append(recall20)\n",
    "        print(f\"******* At {i} TASK recall20 = {recall20}\\n\")\n",
    "\n",
    "print(f\"avg recall : {sum(recall_list)/len(recall_list)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
